{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b508788-fc22-425d-a592-ebdeb8b4352f",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Si hay multiwords, se debe saltar una linea y coger las dos siguientes. Ejemplo:\n",
    "\n",
    "19-20\tdon't\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "\n",
    "19\tdo\tdo\tAUX\tVBP\tMood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\t21\taux\t21:aux\t_\n",
    "\n",
    "20\tn't\tnot\tPART\tRB\tPolarity=Neg\t21\tadvmod\t21:advmod\t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01bab38-f902-4d05-9284-738db4162657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu_data(filepath):\n",
    "    \"\"\"\n",
    "    Carga y procesa un archivo CoNLL-U, extrayendo las oraciones y sus etiquetas UPOS.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # 1. Ignorar comentarios y líneas vacías que no sean separadores de oración\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # 2. Línea en blanco: indica el final de una oración\n",
    "            elif line == '':\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "                    current_sentence = []\n",
    "                    current_tags = []\n",
    "            \n",
    "            # 3. Procesar línea de palabra\n",
    "            else:\n",
    "                fields = line.split('\\t')\n",
    "                \n",
    "                # Ignorar tokens multiword (ID con guion, e.g., '1-2') o nodos vacíos (ID con punto, e.g., '1.1') \n",
    "                if '-' in fields[0] or '.' in fields[0]:\n",
    "                    continue\n",
    "\n",
    "                # Extraer la palabra (FORM, índice 1) y la etiqueta PoS (UPOS, índice 3)\n",
    "                word = fields[1]\n",
    "                pos_tag = fields[3]\n",
    "                \n",
    "                current_sentence.append(word)\n",
    "                current_tags.append(pos_tag)\n",
    "\n",
    "    # Asegurarse de añadir la última oración si el archivo no termina en línea vacía\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15447062-510f-41cc-a7ff-07ffd00199c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'a', 'nice', 'search', 'engine', '.'] ['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./en_ewt-ud-train.conllu\"\n",
    "\n",
    "sentences = []\n",
    "tags = []\n",
    "current_sentence = []\n",
    "current_tags = []\n",
    "\n",
    "with open (filepath, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Ignorar comentarios y lineas vacias\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        # Si hay una linea en blanco indica el final de una oracion\n",
    "        elif line == '':\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                tags.append(current_tags)\n",
    "                current_sentences = []\n",
    "                current_tags = []\n",
    "                \n",
    "        # Procesar línea de palabra\n",
    "        else:\n",
    "            fields = line.split('\\t')\n",
    "\n",
    "            # Ignorar tokens multiword\n",
    "            if '-' in fields[0] or '.' in fields[0]:\n",
    "                    continue\n",
    "                \n",
    "            # Extraer la palabra (FORM, índice 1) y la etiqueta PoS (UPOS, índice 3)\n",
    "            word = fields[1]\n",
    "            pos_tag = fields[3]\n",
    "                \n",
    "            current_sentence.append(word)\n",
    "            current_tags.append(pos_tag)\n",
    "\n",
    "\n",
    "    # Asegurarse de añadir la última oración si el archivo no termina en línea vacía\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    \n",
    "# Ejemplo de uso (asumiendo que los archivos están en la misma carpeta):\n",
    "train_sents, train_tags = load_conllu_data('en_ewt-ud-train.conllu')\n",
    "dev_sents, dev_tags = load_conllu_data('en_ewt-ud-dev.conllu')\n",
    "test_sents, test_tags = load_conllu_data('en_ewt-ud-test.conllu')\n",
    "\n",
    "print(test_sents[5], test_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23542ba7-4baf-48ec-b4de-ce00ba0e29f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'a', 'nice', 'search', 'engine', '.'] ['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./en_ewt-ud-train.conllu\"\n",
    "\n",
    "sentences = []\n",
    "tags = []\n",
    "current_sentence = []\n",
    "current_tags = []\n",
    "\n",
    "with open (filepath, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Ignorar comentarios y lineas vacias\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        # Si hay una linea en blanco indica el final de una oracion\n",
    "        elif line == '':\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                tags.append(current_tags)\n",
    "                current_sentences = []\n",
    "                current_tags = []\n",
    "                \n",
    "        # Procesar línea de palabra\n",
    "        else:\n",
    "            fields = line.split('\\t')\n",
    "\n",
    "            # Ignorar tokens multiword\n",
    "            if '-' in fields[0] or '.' in fields[0]:\n",
    "                    continue\n",
    "                \n",
    "            # Extraer la palabra (FORM, índice 1) y la etiqueta PoS (UPOS, índice 3)\n",
    "            word = fields[1]\n",
    "            pos_tag = fields[3]\n",
    "                \n",
    "            current_sentence.append(word)\n",
    "            current_tags.append(pos_tag)\n",
    "\n",
    "\n",
    "    # Asegurarse de añadir la última oración si el archivo no termina en línea vacía\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    \n",
    "# Ejemplo de uso (asumiendo que los archivos están en la misma carpeta):\n",
    "train_sents, train_tags = load_conllu_data('en_ewt-ud-train.conllu')\n",
    "dev_sents, dev_tags = load_conllu_data('en_ewt-ud-dev.conllu')\n",
    "test_sents, test_tags = load_conllu_data('en_ewt-ud-test.conllu')\n",
    "\n",
    "print(test_sents[5], test_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b84413-2884-46cc-9e88-5cc63fa9dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Text Vectorization: Creating the Dictionaries\n",
    "\n",
    "The first step in preparing the data for the LSTM model is to convert our text-based sentences and tags into numerical sequences. Neural networks can only process numbers, so we need a consistent way to map each word and each tag to a unique integer ID.\n",
    "\n",
    "For this task, we'll use Keras's modern `TextVectorization` layer. We will create two separate instances of this layer: one for the input words (`word_vectorizer`) and one for the output tags (`tag_vectorizer`).\n",
    "\n",
    "The process involves two main stages:\n",
    "1.  **Configuration**: We initialize the `TextVectorization` layer with `output_mode='int'` to ensure it produces sequences of integer IDs (e.g., \"Google is nice\" -> `[2, 3, 42]`). We also set `output_sequence_length=128` to enforce that all sequences are padded or truncated to a fixed length, which is a requirement for the model.\n",
    "2.  **Adaptation**: We then call the `.adapt()` method on our training data. This step builds the internal vocabulary for each vectorizer. It analyzes all the words (or tags) in the training set and assigns a unique integer to each one. This ensures our \"dictionaries\" are based only on the data the model is allowed to learn from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e707d6-13fb-40f5-88a7-4229210c87ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: ['Google', 'is', 'a', 'jojoto', 'engine']\n",
      "\n",
      "example vec:\n",
      "[[2475    9    5    1 1862    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Max lend of words for a sentence\n",
    "MAX_LEN = 128 \n",
    "\n",
    "# Create the TextVectorization layer.\n",
    "word_vectorizer = TextVectorization(\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN\n",
    ")\n",
    "\n",
    "#Flatten the training sentences.\n",
    "train_sents_flat = [' '.join(sentence) for sentence in train_sents]\n",
    "\n",
    "# Adapt the vectorizer to the training data.\n",
    "# This builds the internal vocabulary (the word-to-integer dictionary).\n",
    "word_vectorizer.adapt(train_sents_flat) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Let's test it with an example ---\n",
    "# Create an example sentence containing an unknown word (\"jojoto\").\n",
    "example_sentence = [\"Google\", \"is\", \"a\", \"jojoto\", \"engine\"]\n",
    "print(\"example:\", example_sentence)\n",
    "\n",
    "# running example\n",
    "example_vec = word_vectorizer([\" \".join(example_sentence)])\n",
    "print(\"\\nexample vec:\")\n",
    "print(example_vec.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f502ea4-ee7c-4470-94a3-6e8b806f4d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptation complete. Vocabulary size: 16250\n",
      "Adaptation complete. Vocabulary size: 20\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 16250 words.\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 20 tags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "def create_and_adapt_vectorizer(sentences, max_len=128):\n",
    "\n",
    "    vectorizer = TextVectorization(\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_len\n",
    "    )\n",
    "\n",
    "    sentences_flat = [' '.join(sentence) for sentence in sentences]\n",
    "    \n",
    "    vectorizer.adapt(sentences_flat)\n",
    "    \n",
    "    vocab_size = len(vectorizer.get_vocabulary())\n",
    "    \n",
    "    print(f\"Adaptation complete. Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    return vectorizer, vocab_size\n",
    "\n",
    "# How to use the function ---\n",
    "\n",
    "\n",
    "word_vectorizer, WORD_VOCAB_SIZE = create_and_adapt_vectorizer(train_sents)\n",
    "\n",
    "tags_vectorizer, TAGS_VOCAB_SIZE = create_and_adapt_vectorizer(train_tags_flat)\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {WORD_VOCAB_SIZE} words.\")\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {TAGS_VOCAB_SIZE} tags.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9858691-93ff-41c9-8436-6750835f935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptation complete.\n",
      "Example tag sequence (original):\n",
      "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n",
      "\n",
      "Vectorized tag sequence:\n",
      "[[10  3 10  3  8  2  4 10 10 10  3 10  3  7  2  6  7  2  6  7  2  6 10  3\n",
      "   6  7  8  2  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Create the TextVectorization layer for the tags.\n",
    "tag_vectorizer = TextVectorization(\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN # Tag sequences must have the same length as word sequences.\n",
    ")\n",
    "\n",
    "# Flatten the training tags for the adaptation step.\n",
    "train_tags_flat = [' '.join(tag_list) for tag_list in train_tags]\n",
    "\n",
    "# Adapt the layer to learn the vocabulary of our training tags.\n",
    "tag_vectorizer.adapt(train_tags_flat)\n",
    "print(\"Adaptation complete.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Let's test it with an example ---\n",
    "\n",
    "# 1. Take the first list of tags from our training data as an example.\n",
    "example_tags = train_tags[0]\n",
    "print(\"Example tag sequence (original):\")\n",
    "print(example_tags)\n",
    "\n",
    "# 2. Vectorize the example tag sequence.\n",
    "# We must join it into a single string and pass it as a list.\n",
    "example_tags_vec = tag_vectorizer([\" \".join(example_tags)])\n",
    "\n",
    "# 3. Print the resulting numerical sequence.\n",
    "# Notice how the output is a sequence of 128 numbers, padded with 0s at the end.\n",
    "print(\"\\nVectorized tag sequence:\")\n",
    "print(example_tags_vec.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c66169e-26d6-48c3-92a6-fd32ac5065dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario de palabras: 16250\n",
      "Tamaño del vocabulario de etiquetas: 19\n",
      "Algunas etiquetas del vocabulario: ['', '[UNK]', np.str_('noun'), np.str_('punct'), np.str_('verb'), np.str_('pron'), np.str_('adp'), np.str_('det'), np.str_('adj'), np.str_('aux')]\n"
     ]
    }
   ],
   "source": [
    "# get the vocab and tags\n",
    "word_vocab = word_vectorizer.get_vocabulary()\n",
    "tag_vocab = tag_vectorizer.get_vocabulary()\n",
    "\n",
    "# het the sizes\n",
    "WORD_VOCAB_SIZE = len(word_vocab)\n",
    "TAG_VOCAB_SIZE = len(tag_vocab)\n",
    "print(TAG_VOCAB_SIZE)\n",
    "print(f\"size of the worids: {WORD_VOCAB_SIZE}\")\n",
    "print(f\"size of the tags: {TAG_VOCAB_SIZE}\")\n",
    "print(f\"some tags: {tag_vocab[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59265d89-4bc5-4ed7-b03d-4dd4c8f45afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "esto es el modeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c167bc80-153b-4352-b1fd-6ac12f6b8b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_2              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importaciones adicionales para construir el modelo\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed\n",
    "\n",
    "# Hiperparámetros del modelo (puedes experimentar con estos valores)\n",
    "EMBEDDING_DIM = 64\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "# Construcción del modelo\n",
    "model = Sequential([\n",
    "    # 1. Capa de Embedding: Convierte IDs de palabras en vectores de significado\n",
    "    Embedding(input_dim=WORD_VOCAB_SIZE, \n",
    "              output_dim=EMBEDDING_DIM, \n",
    "              mask_zero=True), # mask_zero=True le dice al modelo que ignore los '0' del padding\n",
    "\n",
    "    # 2. Capa LSTM: Procesa la secuencia de embeddings y recuerda el contexto\n",
    "    LSTM(units=LSTM_UNITS, \n",
    "         return_sequences=True), # ¡Crucial! Devuelve una salida para cada palabra\n",
    "\n",
    "    # 3. Capa de Salida: Aplica un clasificador a cada palabra de la secuencia\n",
    "    TimeDistributed(Dense(units=TAG_VOCAB_SIZE, activation='softmax'))\n",
    "])\n",
    "\n",
    "# Imprimir un resumen de la arquitectura\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d26c495-7270-4b16-9be6-dbf849176014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3d1fc2b-b956-4ced-9681-5fdd80d3d81d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m BATCH_SIZE = \u001b[32m64\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[32m      6\u001b[39m history = model.fit(\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mX_train\u001b[49m, \n\u001b[32m      8\u001b[39m     y_train,\n\u001b[32m      9\u001b[39m     epochs=EPOCHS,\n\u001b[32m     10\u001b[39m     batch_size=BATCH_SIZE,\n\u001b[32m     11\u001b[39m     validation_data=(X_dev, y_dev)\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Parámetros de entrenamiento\n",
    "EPOCHS = 5 # Empezamos con pocas para probar rápido\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_dev, y_dev)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53a933-7fd4-4db8-a6d6-96493aa81f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
