{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b508788-fc22-425d-a592-ebdeb8b4352f",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Si hay multiwords, se debe saltar una linea y coger las dos siguientes. Ejemplo:\n",
    "\n",
    "19-20\tdon't\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "\n",
    "19\tdo\tdo\tAUX\tVBP\tMood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\t21\taux\t21:aux\t_\n",
    "\n",
    "20\tn't\tnot\tPART\tRB\tPolarity=Neg\t21\tadvmod\t21:advmod\t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e01bab38-f902-4d05-9284-738db4162657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu_data(filepath):\n",
    "    \"\"\"\n",
    "    Carga y procesa un archivo CoNLL-U, extrayendo las oraciones y sus etiquetas UPOS.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # 1. Ignorar comentarios y líneas vacías que no sean separadores de oración\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # 2. Línea en blanco: indica el final de una oración\n",
    "            elif line == '':\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "                    current_sentence = []\n",
    "                    current_tags = []\n",
    "            \n",
    "            # 3. Procesar línea de palabra\n",
    "            else:\n",
    "                fields = line.split('\\t')\n",
    "                \n",
    "                # Ignorar tokens multiword (ID con guion, e.g., '1-2') o nodos vacíos (ID con punto, e.g., '1.1') \n",
    "                if '-' in fields[0] or '.' in fields[0]:\n",
    "                    continue\n",
    "\n",
    "                # Extraer la palabra (FORM, índice 1) y la etiqueta PoS (UPOS, índice 3)\n",
    "                word = fields[1]\n",
    "                pos_tag = fields[3]\n",
    "                \n",
    "                current_sentence.append(word)\n",
    "                current_tags.append(pos_tag)\n",
    "\n",
    "    # Asegurarse de añadir la última oración si el archivo no termina en línea vacía\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15447062-510f-41cc-a7ff-07ffd00199c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'a', 'nice', 'search', 'engine', '.'] ['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./en_ewt-ud-train.conllu\"\n",
    "\n",
    "sentences = []\n",
    "tags = []\n",
    "current_sentence = []\n",
    "current_tags = []\n",
    "\n",
    "with open (filepath, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Ignorar comentarios y lineas vacias\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        # Si hay una linea en blanco indica el final de una oracion\n",
    "        elif line == '':\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                tags.append(current_tags)\n",
    "                current_sentences = []\n",
    "                current_tags = []\n",
    "                \n",
    "        # Procesar línea de palabra\n",
    "        else:\n",
    "            fields = line.split('\\t')\n",
    "\n",
    "            # Ignorar tokens multiword\n",
    "            if '-' in fields[0] or '.' in fields[0]:\n",
    "                    continue\n",
    "                \n",
    "            # Extraer la palabra (FORM, índice 1) y la etiqueta PoS (UPOS, índice 3)\n",
    "            word = fields[1]\n",
    "            pos_tag = fields[3]\n",
    "                \n",
    "            current_sentence.append(word)\n",
    "            current_tags.append(pos_tag)\n",
    "\n",
    "\n",
    "    # Asegurarse de añadir la última oración si el archivo no termina en línea vacía\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    \n",
    "# Ejemplo de uso (asumiendo que los archivos están en la misma carpeta):\n",
    "train_sents, train_tags = load_conllu_data('en_ewt-ud-train.conllu')\n",
    "dev_sents, dev_tags = load_conllu_data('en_ewt-ud-dev.conllu')\n",
    "test_sents, test_tags = load_conllu_data('en_ewt-ud-test.conllu')\n",
    "\n",
    "print(test_sents[5], test_tags[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab4cc4-3e45-4021-8f89-c021ba5db215",
   "metadata": {},
   "source": [
    "## 2. Text Vectorization: Creating the Dictionaries\n",
    "\n",
    "The first step in preparing the data for the LSTM model is to convert our text-based sentences and tags into numerical sequences. Neural networks can only process numbers, so we need a consistent way to map each word and each tag to a unique integer ID.\n",
    "\n",
    "For this task, we'll use Keras's modern `TextVectorization` layer. We will create two separate instances of this layer: one for the input words (`word_vectorizer`) and one for the output tags (`tag_vectorizer`).\n",
    "\n",
    "The process involves two main stages:\n",
    "1.  **Configuration**: We initialize the `TextVectorization` layer with `output_mode='int'` to ensure it produces sequences of integer IDs (e.g., \"Google is nice\" -> `[2, 3, 42]`). We also set `output_sequence_length=128` to enforce that all sequences are padded or truncated to a fixed length, which is a requirement for the model.\n",
    "2.  **Adaptation**: We then call the `.adapt()` method on our training data. This step builds the internal vocabulary for each vectorizer. It analyzes all the words (or tags) in the training set and assigns a unique integer to each one. This ensures our \"dictionaries\" are based only on the data the model is allowed to learn from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e707d6-13fb-40f5-88a7-4229210c87ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: ['Google', 'is', 'a', 'jojoto', 'engine']\n",
      "\n",
      "example vec:\n",
      "[[2475    9    5    1 1862    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Max lend of words for a sentence\n",
    "MAX_LEN = 128 \n",
    "\n",
    "# Create the TextVectorization layer.\n",
    "word_vectorizer = TextVectorization(\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN\n",
    ")\n",
    "\n",
    "#Flatten the training sentences.\n",
    "train_sents_flat = [' '.join(sentence) for sentence in train_sents]\n",
    "\n",
    "# Flatten the training tags for the adaptation step.\n",
    "train_tags_flat = [' '.join(tag_list) for tag_list in train_tags]\n",
    "\n",
    "# Adapt the vectorizer to the training data.\n",
    "# This builds the internal vocabulary (the word-to-integer dictionary).\n",
    "word_vectorizer.adapt(train_sents_flat) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Let's test it with an example ---\n",
    "# Create an example sentence containing an unknown word (\"jojoto\").\n",
    "example_sentence = [\"Google\", \"is\", \"a\", \"jojoto\", \"engine\"]\n",
    "print(\"example:\", example_sentence)\n",
    "\n",
    "# running example\n",
    "example_vec = word_vectorizer([\" \".join(example_sentence)])\n",
    "print(\"\\nexample vec:\")\n",
    "print(example_vec.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2915196b-9542-479f-a879-e8acf9dd8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptation complete. Vocabulary size: 16250\n",
      "Adaptation complete. Vocabulary size: 19\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 16250 words.\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 19 tags.\n",
      "Vectorizing all data sets...\n",
      "Vectorization complete!\n",
      "\n",
      "Shape of X_train: (12544, 128)\n",
      "Shape of y_train: (12544, 128)\n",
      "Shape of X_dev: (2001, 128)\n",
      "Shape of y_dev: (2001, 128)\n",
      "tf.Tensor(\n",
      "[  169  4754   258   637  1119  4039 15673   169 15421     2  6883    30\n",
      "     2  7141     8     2   436     6  6811   721     2  2132  1607     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0], shape=(128,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[ 6  7 10  4  7  2  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0], shape=(128,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "def create_and_adapt_vectorizer(sentences, max_len=128):\n",
    "\n",
    "    vectorizer = TextVectorization(\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_len\n",
    "    )\n",
    "\n",
    "    sentences_flat = [' '.join(sentence) for sentence in sentences]\n",
    "    \n",
    "    vectorizer.adapt(sentences_flat)\n",
    "    \n",
    "    vocab_size = len(vectorizer.get_vocabulary())\n",
    "    \n",
    "    print(f\"Adaptation complete. Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    return vectorizer, vocab_size\n",
    "\n",
    "# How to use the function ---\n",
    "\n",
    "\n",
    "word_vectorizer, WORD_VOCAB_SIZE = create_and_adapt_vectorizer(train_sents)\n",
    "\n",
    "tags_vectorizer, TAGS_VOCAB_SIZE = create_and_adapt_vectorizer(train_tags)\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {WORD_VOCAB_SIZE} words.\")\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {TAGS_VOCAB_SIZE} tags.\")\n",
    "\n",
    "print(\"Vectorizing all data sets...\")\n",
    "\n",
    "# --- 1. Flatten the data from list of lists to list of strings ---\n",
    "# The vectorizer layers expect a flat list of strings as input.\n",
    "train_sents_flat = [' '.join(sentence) for sentence in train_sents]\n",
    "train_tags_flat = [' '.join(tag_list) for tag_list in train_tags]\n",
    "\n",
    "dev_sents_flat = [' '.join(sentence) for sentence in dev_sents]\n",
    "dev_tags_flat = [' '.join(tag_list) for tag_list in dev_tags]\n",
    "\n",
    "test_sents_flat = [' '.join(sentence) for sentence in test_sents]\n",
    "test_tags_flat = [' '.join(tag_list) for tag_list in test_tags]\n",
    "\n",
    "\n",
    "# --- 2. Use the vectorizers to transform the flattened data ---\n",
    "# Now we call the vectorizers with the correct input format.\n",
    "X_train = word_vectorizer(train_sents_flat)\n",
    "y_train = tags_vectorizer(train_tags_flat)\n",
    "\n",
    "X_dev = word_vectorizer(dev_sents_flat)\n",
    "y_dev = tags_vectorizer(dev_tags_flat)\n",
    "\n",
    "X_test = word_vectorizer(test_sents_flat)\n",
    "y_test = tags_vectorizer(test_tags_flat)\n",
    "\n",
    "print(\"Vectorization complete!\")\n",
    "print(\"\\nShape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_dev:\", X_dev.shape)\n",
    "print(\"Shape of y_dev:\", y_dev.shape)\n",
    "\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_dev[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe573b4-8198-4fd8-b6e2-bffa1628e6ba",
   "metadata": {},
   "source": [
    "## Esto es el modeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f7f76f-f98e-4724-a226-e5c56c22c346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pos_tagger_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"pos_tagger_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_ids_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080,000</span> │ word_ids_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ word_ids_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_tag_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,451</span> │ lstm_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_ids_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m2,080,000\u001b[0m │ word_ids_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ word_ids_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │    \u001b[38;5;34m131,584\u001b[0m │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_tag_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m19\u001b[0m)   │      \u001b[38;5;34m2,451\u001b[0m │ lstm_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,214,035</span> (8.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,214,035\u001b[0m (8.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,214,035</span> (8.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,214,035\u001b[0m (8.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# --- Hiperparámetros (igual que antes) ---\n",
    "# (Asegúrate de tener estas variables definidas de pasos anteriores)\n",
    "# WORD_VOCAB_SIZE, TAG_VOCAB_SIZE, MAX_LEN\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_UNITS = 128\n",
    "\n",
    "# --- Construcción del Modelo con la API Funcional ---\n",
    "\n",
    "# 1. Definir la capa de Entrada \n",
    "# Le decimos al modelo que recibirá secuencias de números enteros de longitud MAX_LEN.\n",
    "inputs = Input(shape=(MAX_LEN,), name='word_ids_input')\n",
    "\n",
    "# 2. Conectar las capas en una \"carrera de relevos\" 🔗\n",
    "# La capa Embedding recibe los 'inputs' y su salida se guarda en 'x'.\n",
    "x = Embedding(\n",
    "    input_dim=WORD_VOCAB_SIZE, \n",
    "    output_dim=EMBEDDING_DIM, \n",
    "    mask_zero=True, # Importante para que ignore el padding\n",
    "    name='word_embedding'\n",
    ")(inputs)\n",
    "\n",
    "# La capa LSTM recibe la salida del Embedding ('x') y su propia salida se guarda de nuevo en 'x'.\n",
    "x = LSTM(\n",
    "    units=LSTM_UNITS, \n",
    "    return_sequences=True, # Necesitamos una salida para cada palabra\n",
    "    name='lstm_layer'\n",
    ")(x)\n",
    "\n",
    "# La capa TimeDistributed(Dense) recibe la salida de la LSTM ('x') y su salida es la final.\n",
    "# La llamamos 'outputs' para que quede claro que es el final del camino.\n",
    "outputs = TimeDistributed(\n",
    "    Dense(units=TAGS_VOCAB_SIZE, activation='softmax'), \n",
    "    name='pos_tag_output'\n",
    ")(x)\n",
    "\n",
    "# 3. Crear el Modelo final\n",
    "# Le decimos a Keras dónde empieza el modelo (inputs) y dónde termina (outputs).\n",
    "model = Model(inputs=inputs, outputs=outputs, name='pos_tagger_model')\n",
    "\n",
    "# ¡Listo! Ahora puedes imprimir el resumen y ver la arquitectura.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d26c495-7270-4b16-9be6-dbf849176014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
    "\n",
    "def custom_masked_accuracy(y_true, y_pred):\n",
    "    # 1. Crea una máscara que es 0.0 para padding (ID 0) y 1.0 para todo lo demás\n",
    "    # tf.not_equal(y_true, 0) crea un tensor de Boleanos (True/False)\n",
    "    # tf.cast lo convierte a 1.0 y 0.0\n",
    "    sample_weight = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
    "\n",
    "    # 2. Calcula la precisión para TODOS los tokens (incluyendo padding)\n",
    "    accuracy_all_tokens = sparse_categorical_accuracy(y_true, y_pred)\n",
    "    \n",
    "    # 3. Aplica la máscara: multiplica la precisión de cada token por 1.0 o 0.0\n",
    "    weighted_accuracy = accuracy_all_tokens * sample_weight\n",
    "    \n",
    "    # 4. Calcula la media solo sobre los tokens reales\n",
    "    # Suma las precisiones (solo los 1.0) y divide por el número de tokens reales (la suma de la máscara)\n",
    "    # Añadimos epsilon para evitar una división por cero si un batch estuviera vacío\n",
    "    epsilon = 1e-7\n",
    "    return tf.reduce_sum(weighted_accuracy) / (tf.reduce_sum(sample_weight) + epsilon)\n",
    "\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy', # Esto ya estaba bien\n",
    "    metrics=[custom_masked_accuracy]       # <-- ¡USA LA MÉTRICA PERSONALIZADA!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3d1fc2b-b956-4ced-9681-5fdd80d3d81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 13:56:52.976709: E tensorflow/core/util/util.cc:131] oneDNN supports DT_BOOL only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 355ms/step - custom_masked_accuracy: 0.2968 - loss: 2.2786 - val_custom_masked_accuracy: 0.4304 - val_loss: 1.7450\n",
      "Epoch 2/5\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 348ms/step - custom_masked_accuracy: 0.4566 - loss: 1.5883 - val_custom_masked_accuracy: 0.5349 - val_loss: 1.3559\n",
      "Epoch 3/5\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 341ms/step - custom_masked_accuracy: 0.5242 - loss: 1.2993 - val_custom_masked_accuracy: 0.5569 - val_loss: 1.2289\n",
      "Epoch 4/5\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 344ms/step - custom_masked_accuracy: 0.5658 - loss: 1.1286 - val_custom_masked_accuracy: 0.5580 - val_loss: 1.1983\n",
      "Epoch 5/5\n",
      "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 348ms/step - custom_masked_accuracy: 0.5966 - loss: 1.0035 - val_custom_masked_accuracy: 0.5543 - val_loss: 1.2071\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de entrenamiento\n",
    "EPOCHS = 5 # Empezamos con pocas para probar rápido\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_dev, y_dev)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b415b0c8-f607-4391-ab6b-d6be2e55f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Asumimos que tu 'model', 'word_vectorizer', y 'tags_vectorizer' \n",
    "# ya están definidos y entrenados en las celdas anteriores.\n",
    "\n",
    "# 1. Obtenemos el vocabulario de etiquetas (la lista de strings)\n",
    "# El ID 0 es '', el ID 1 es '[UNK]', así que el vocabulario real empieza en el índice 2\n",
    "tags_vocab = tags_vectorizer.get_vocabulary()\n",
    "\n",
    "def predict_tags(sentence_string):\n",
    "    \"\"\"\n",
    "    Toma una frase (string), la procesa con el modelo \n",
    "    y muestra las predicciones palabra por palabra.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"--- Prediciendo para: ---\\n{sentence_string}\\n\")\n",
    "    \n",
    "    # 0. Separamos las palabras para saber cuántas son\n",
    "    words = sentence_string.split(' ')\n",
    "    \n",
    "    # 1. Convertir la frase (string) en un tensor de IDs (shape [1, 128])\n",
    "    # El vectorizador espera una lista de strings\n",
    "    input_tensor = word_vectorizer([sentence_string])\n",
    "    \n",
    "    # 2. Obtener las predicciones del modelo\n",
    "    # El modelo devuelve probabilidades (shape [1, 128, 20])\n",
    "    predictions = model.predict(input_tensor)\n",
    "    \n",
    "    # 3. Encontrar el ID de la etiqueta con mayor probabilidad para cada token\n",
    "    # Usamos np.argmax para obtener los IDs ganadores (shape [1, 128])\n",
    "    predicted_ids = np.argmax(predictions, axis=-1)[0] # [0] para coger el primer (y único) batch\n",
    "    \n",
    "    # 4. Mostrar los resultados\n",
    "    print(\"--- Resultados: ---\")\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        tag_id = predicted_ids[i]\n",
    "        tag_name = tags_vocab[tag_id]\n",
    "        \n",
    "        print(f\"{word:<15} -> {tag_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef0d882-0606-4d87-8886-33d8c7e43a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prediciendo para: ---\n",
      "Google is a nice search engine .\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945ms/step\n",
      "--- Resultados: ---\n",
      "Google          -> propn\n",
      "is              -> aux\n",
      "a               -> det\n",
      "nice            -> adj\n",
      "search          -> noun\n",
      "engine          -> noun\n",
      ".               -> noun\n",
      "--- Prediciendo para: ---\n",
      "I am writing this new sentence for the test .\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "--- Resultados: ---\n",
      "I               -> pron\n",
      "am              -> aux\n",
      "writing         -> verb\n",
      "this            -> det\n",
      "new             -> adj\n",
      "sentence        -> noun\n",
      "for             -> adp\n",
      "the             -> det\n",
      "test            -> noun\n",
      ".               -> noun\n",
      "--- Prediciendo para: ---\n",
      "The university is in ntiago .\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "--- Resultados: ---\n",
      "The             -> det\n",
      "university      -> propn\n",
      "is              -> aux\n",
      "in              -> adp\n",
      "ntiago          -> propn\n",
      ".               -> propn\n"
     ]
    }
   ],
   "source": [
    "# Prueba 1: La frase de ejemplo de la práctica\n",
    "predict_tags(\"Google is a nice search engine .\")\n",
    "\n",
    "# Prueba 2: Una frase nueva\n",
    "predict_tags(\"I am writing this new sentence for the test .\")\n",
    "\n",
    "# Prueba 3: Otra más\n",
    "predict_tags(\"The university is in ntiago .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189ae9f-5ff1-4520-a7a9-522414995a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
