{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b508788-fc22-425d-a592-ebdeb8b4352f",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Si hay multiwords, se debe saltar una linea y coger las dos siguientes. Ejemplo:\n",
    "\n",
    "19-20\tdon't\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "\n",
    "19\tdo\tdo\tAUX\tVBP\tMood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\t21\taux\t21:aux\t_\n",
    "\n",
    "20\tn't\tnot\tPART\tRB\tPolarity=Neg\t21\tadvmod\t21:advmod\t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e01bab38-f902-4d05-9284-738db4162657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu_data(filepath):\n",
    "    \"\"\"\n",
    "    Carga y procesa un archivo CoNLL-U, extrayendo las oraciones y sus etiquetas UPOS.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # 1. Ignorar comentarios y líneas vacías que no sean separadores de oración\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # 2. Línea en blanco: indica el final de una oración\n",
    "            elif line == '':\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "                    current_sentence = []\n",
    "                    current_tags = []\n",
    "            \n",
    "            # 3. Procesar línea de palabra\n",
    "            else:\n",
    "                fields = line.split('\\t')\n",
    "                \n",
    "                # Ignorar tokens multiword (ID con guion, e.g., '1-2') o nodos vacíos (ID con punto, e.g., '1.1') \n",
    "                if '-' in fields[0] or '.' in fields[0]:\n",
    "                    continue\n",
    "\n",
    "                # Extraer la palabra (FORM, índice 1) y la etiqueta PoS (UPOS, índice 3)\n",
    "                word = fields[1]\n",
    "                pos_tag = fields[3]\n",
    "                \n",
    "                current_sentence.append(word)\n",
    "                current_tags.append(pos_tag)\n",
    "\n",
    "    # Asegurarse de añadir la última oración si el archivo no termina en línea vacía\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15447062-510f-41cc-a7ff-07ffd00199c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'a', 'nice', 'search', 'engine', '.'] ['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    " \n",
    "train_sents, train_tags = load_conllu_data('en_ewt-ud-train.conllu')\n",
    "dev_sents, dev_tags = load_conllu_data('en_ewt-ud-dev.conllu')\n",
    "test_sents, test_tags = load_conllu_data('en_ewt-ud-test.conllu')\n",
    "\n",
    "print(test_sents[5], test_tags[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab4cc4-3e45-4021-8f89-c021ba5db215",
   "metadata": {},
   "source": [
    "## 2. Text Vectorization: Creating the Dictionaries\n",
    "\n",
    "The first step in preparing the data for the LSTM model is to convert our text-based sentences and tags into numerical sequences. Neural networks can only process numbers, so we need a consistent way to map each word and each tag to a unique integer ID.\n",
    "\n",
    "For this task, we'll use Keras's modern `TextVectorization` layer. We will create two separate instances of this layer: one for the input words (`word_vectorizer`) and one for the output tags (`tag_vectorizer`).\n",
    "\n",
    "The process involves two main stages:\n",
    "1.  **Configuration**: We initialize the `TextVectorization` layer with `output_mode='int'` to ensure it produces sequences of integer IDs (e.g., \"Google is nice\" -> `[2, 3, 42]`). We also set `output_sequence_length=128` to enforce that all sequences are padded or truncated to a fixed length, which is a requirement for the model.\n",
    "2.  **Adaptation**: We then call the `.adapt()` method on our training data. This step builds the internal vocabulary for each vectorizer. It analyzes all the words (or tags) in the training set and assigns a unique integer to each one. This ensures our \"dictionaries\" are based only on the data the model is allowed to learn from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2915196b-9542-479f-a879-e8acf9dd8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptation complete. Vocabulary size: 19676\n",
      "Adaptation complete. Vocabulary size: 19\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 19676 words.\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 19 tags.\n",
      "Vectorizing all data sets...\n",
      "Vectorization complete!\n",
      "\n",
      "Shape of X_train: (12544, 128)\n",
      "Shape of y_train: (12544, 128)\n",
      "Shape of X_dev: (2001, 128)\n",
      "Shape of y_dev: (2001, 128)\n",
      "tf.Tensor(\n",
      "[  263    16  6153    45   293   709  1150  4859 18887   592    16 18793\n",
      "     4     3  7336    35     3  7510    10     3   486     8  8931     4\n",
      "   751     3  2355  1642     2     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0], shape=(128,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[10  3 10  3  8  2  4 10 10 10  3 10  3  7  2  6  7  2  6  7  2  6 10  3\n",
      "  6  7  8  2  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0], shape=(128,), dtype=int64)\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "def create_and_adapt_vectorizer(sentences, max_len=128, standardize='lower_and_strip_punctuation'):\n",
    "\n",
    "    vectorizer = TextVectorization(\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_len,\n",
    "        standardize=standardize\n",
    "    )\n",
    "\n",
    "    sentences_flat = [' '.join(sentence) for sentence in sentences]\n",
    "    \n",
    "    vectorizer.adapt(sentences_flat)\n",
    "    \n",
    "    vocab_size = len(vectorizer.get_vocabulary())\n",
    "    \n",
    "    print(f\"Adaptation complete. Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    return vectorizer, vocab_size\n",
    "\n",
    "\n",
    "# 2. Pasa 'standardize=None' al vectorizador de palabras\n",
    "word_vectorizer, WORD_VOCAB_SIZE = create_and_adapt_vectorizer(train_sents, standardize=None)\n",
    "\n",
    "# 3. El vectorizador de etiquetas déjalo como estaba (no necesita el cambio)\n",
    "tags_vectorizer, TAGS_VOCAB_SIZE = create_and_adapt_vectorizer(train_tags)\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {WORD_VOCAB_SIZE} words.\")\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {TAGS_VOCAB_SIZE} tags.\")\n",
    "\n",
    "print(\"Vectorizing all data sets...\")\n",
    "\n",
    "# --- 1. Flatten the data from list of lists to list of strings ---\n",
    "# The vectorizer layers expect a flat list of strings as input.\n",
    "train_sents_flat = [' '.join(sentence) for sentence in train_sents]\n",
    "train_tags_flat = [' '.join(tag_list) for tag_list in train_tags]\n",
    "\n",
    "dev_sents_flat = [' '.join(sentence) for sentence in dev_sents]\n",
    "dev_tags_flat = [' '.join(tag_list) for tag_list in dev_tags]\n",
    "\n",
    "test_sents_flat = [' '.join(sentence) for sentence in test_sents]\n",
    "test_tags_flat = [' '.join(tag_list) for tag_list in test_tags]\n",
    "\n",
    "\n",
    "# --- 2. Use the vectorizers to transform the flattened data ---\n",
    "# Now we call the vectorizers with the correct input format.\n",
    "X_train = word_vectorizer(train_sents_flat)\n",
    "y_train = tags_vectorizer(train_tags_flat)\n",
    "\n",
    "X_dev = word_vectorizer(dev_sents_flat)\n",
    "y_dev = tags_vectorizer(dev_tags_flat)\n",
    "\n",
    "X_test = word_vectorizer(test_sents_flat)\n",
    "y_test = tags_vectorizer(test_tags_flat)\n",
    "\n",
    "print(\"Vectorization complete!\")\n",
    "print(\"\\nShape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_dev:\", X_dev.shape)\n",
    "print(\"Shape of y_dev:\", y_dev.shape)\n",
    "\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_train[0])\n",
    "print(TAGS_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe573b4-8198-4fd8-b6e2-bffa1628e6ba",
   "metadata": {},
   "source": [
    "## Esto es el modeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "39f7f76f-f98e-4724-a226-e5c56c22c346",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pos_tagger_model_v2_bidirectional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"pos_tagger_model_v2_bidirectional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_ids_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,259,264</span> │ word_ids_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_7         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ word_ids_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_12    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │ dropout_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_lstm_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_12… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_13    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │ dropout_lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_lstm_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_13… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_tag_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,451</span> │ dropout_lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ not_equal_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_ids_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │  \u001b[38;5;34m1,259,264\u001b[0m │ word_ids_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_7         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ word_ids_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_12    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m66,048\u001b[0m │ dropout_embeddin… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_lstm_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional_12… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_13    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m98,816\u001b[0m │ dropout_lstm_1[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_lstm_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ bidirectional_13… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_tag_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m19\u001b[0m)   │      \u001b[38;5;34m2,451\u001b[0m │ dropout_lstm_2[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ not_equal_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,426,579</span> (5.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,426,579\u001b[0m (5.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,426,579</span> (5.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,426,579\u001b[0m (5.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Importamos las capas extra que necesitamos: Bidirectional y Dropout\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "LSTM_UNITS = 64\n",
    "DROPOUT_RATE = 0.3\n",
    "\n",
    "\n",
    "# 1. Definir la capa de Entrada\n",
    "inputs = Input(shape=(MAX_LEN,), name='word_ids_input')\n",
    "\n",
    "# 2. Conectar las capas\n",
    "# La capa Embedding recibe los 'inputs'\n",
    "x = Embedding(\n",
    "    input_dim=WORD_VOCAB_SIZE, \n",
    "    output_dim=EMBEDDING_DIM, \n",
    "    mask_zero=True, \n",
    "    name='word_embedding'\n",
    ")(inputs)\n",
    "\n",
    "# Añadimos Dropout después del embedding\n",
    "x = Dropout(DROPOUT_RATE, name='dropout_embedding')(x)\n",
    "\n",
    "\n",
    "# 3. Primera capa Bidirectional LSTM\n",
    "x = Bidirectional(\n",
    "    LSTM(\n",
    "        units=LSTM_UNITS, \n",
    "        return_sequences=True,\n",
    "        name='bidirectional_lstm_1'\n",
    "    )\n",
    ")(x)\n",
    "\n",
    "# Añadimos Dropout después de la primera capa Bi-LSTM\n",
    "x = Dropout(DROPOUT_RATE, name='dropout_lstm_1')(x)\n",
    "\n",
    "# 4. Segunda capa Bidirectional LSTM\n",
    "x = Bidirectional(\n",
    "    LSTM(\n",
    "        units=LSTM_UNITS, \n",
    "        return_sequences=True,\n",
    "        name='bidirectional_lstm_2'\n",
    "    )\n",
    ")(x)\n",
    "\n",
    "# Dropout después de la segunda capa Bi-LSTM\n",
    "x = Dropout(DROPOUT_RATE, name='dropout_lstm_2')(x)\n",
    "\n",
    "\n",
    "# 5. Capa de Salida (igual que antes)\n",
    "outputs = TimeDistributed(\n",
    "    Dense(units=TAGS_VOCAB_SIZE, activation='softmax'), \n",
    "    name='pos_tag_output'\n",
    ")(x)\n",
    "\n",
    "# 6. Crear el Modelo final\n",
    "model = Model(inputs=inputs, outputs=outputs, name='pos_tagger_model_v2_bidirectional')\n",
    "\n",
    "# ¡Listo! Ahora puedes imprimir el resumen y ver la nueva arquitectura\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6d26c495-7270-4b16-9be6-dbf849176014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import sparse_categorical_accuracy\n",
    "\n",
    "def custom_masked_accuracy(y_true, y_pred):\n",
    "    # 1. Crea una máscara que es 0.0 para padding (ID 0) y 1.0 para todo lo demás\n",
    "    sample_weight = tf.cast(tf.not_equal(y_true, 0), dtype=tf.float32)\n",
    "\n",
    "    # 2. Calcula la precisión para todos los tokens (incluyendo padding)\n",
    "    accuracy_all_tokens = sparse_categorical_accuracy(y_true, y_pred)\n",
    "    \n",
    "    # 3. Aplica la máscara: multiplica la precisión de cada token por 1.0 o 0.0\n",
    "    weighted_accuracy = accuracy_all_tokens * sample_weight\n",
    "    \n",
    "    # 4. Calcula la media solo sobre los tokens reales\n",
    "    epsilon = 1e-7\n",
    "    return tf.reduce_sum(weighted_accuracy) / (tf.reduce_sum(sample_weight) + epsilon)\n",
    "\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[custom_masked_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84ac4c40-0dd6-49ac-9f17-16e686422b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 138ms/step - custom_masked_accuracy: 0.4365 - loss: 1.7990 - val_custom_masked_accuracy: 0.7873 - val_loss: 0.7450\n",
      "Epoch 2/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 134ms/step - custom_masked_accuracy: 0.8633 - loss: 0.4844 - val_custom_masked_accuracy: 0.8864 - val_loss: 0.3971\n",
      "Epoch 3/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 133ms/step - custom_masked_accuracy: 0.9293 - loss: 0.2626 - val_custom_masked_accuracy: 0.8938 - val_loss: 0.3627\n",
      "Epoch 4/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 133ms/step - custom_masked_accuracy: 0.9490 - loss: 0.1899 - val_custom_masked_accuracy: 0.8963 - val_loss: 0.3728\n",
      "Epoch 5/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 134ms/step - custom_masked_accuracy: 0.9568 - loss: 0.1573 - val_custom_masked_accuracy: 0.9024 - val_loss: 0.3563\n",
      "Epoch 6/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 131ms/step - custom_masked_accuracy: 0.9625 - loss: 0.1355 - val_custom_masked_accuracy: 0.9018 - val_loss: 0.3661\n",
      "Epoch 7/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 131ms/step - custom_masked_accuracy: 0.9659 - loss: 0.1224 - val_custom_masked_accuracy: 0.9035 - val_loss: 0.3817\n",
      "Epoch 8/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 134ms/step - custom_masked_accuracy: 0.9681 - loss: 0.1118 - val_custom_masked_accuracy: 0.9057 - val_loss: 0.3598\n",
      "Epoch 9/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 139ms/step - custom_masked_accuracy: 0.9710 - loss: 0.1009 - val_custom_masked_accuracy: 0.9059 - val_loss: 0.3590\n",
      "Epoch 10/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 138ms/step - custom_masked_accuracy: 0.9732 - loss: 0.0941 - val_custom_masked_accuracy: 0.9079 - val_loss: 0.3582\n",
      "Epoch 11/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 135ms/step - custom_masked_accuracy: 0.9749 - loss: 0.0872 - val_custom_masked_accuracy: 0.9090 - val_loss: 0.3653\n",
      "Epoch 12/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 136ms/step - custom_masked_accuracy: 0.9765 - loss: 0.0811 - val_custom_masked_accuracy: 0.9084 - val_loss: 0.3714\n",
      "Epoch 13/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 139ms/step - custom_masked_accuracy: 0.9776 - loss: 0.0756 - val_custom_masked_accuracy: 0.9090 - val_loss: 0.3523\n",
      "Epoch 14/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 136ms/step - custom_masked_accuracy: 0.9785 - loss: 0.0719 - val_custom_masked_accuracy: 0.9096 - val_loss: 0.3637\n",
      "Epoch 15/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 138ms/step - custom_masked_accuracy: 0.9801 - loss: 0.0670 - val_custom_masked_accuracy: 0.9095 - val_loss: 0.3848\n",
      "Epoch 16/50\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 138ms/step - custom_masked_accuracy: 0.9809 - loss: 0.0640 - val_custom_masked_accuracy: 0.9087 - val_loss: 0.3907\n"
     ]
    }
   ],
   "source": [
    "# 1. Importa el Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 2. Crea una instancia del callback\n",
    "early_stopping_callback_acc = EarlyStopping(\n",
    "    monitor='val_custom_masked_accuracy', \n",
    "    patience=5,\n",
    "    min_delta=0.001,          # Una mejora de 0.1% \n",
    "    mode='max',               # Queremos maximizar la precisión (accuracy)\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# 3. Pasa el callback a model.fit()\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_dev, y_dev),\n",
    "    callbacks=[early_stopping_callback_acc] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b415b0c8-f607-4391-ab6b-d6be2e55f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Obtenemos el vocabulario de etiquetas (la lista de strings)\n",
    "# El ID 0 es '', el ID 1 es '[UNK]', así que el vocabulario real empieza en el índice 2\n",
    "tags_vocab = tags_vectorizer.get_vocabulary()\n",
    "\n",
    "def predict_tags(sentence_string):\n",
    "    \"\"\"\n",
    "    Toma una frase (string), la procesa con el modelo \n",
    "    y muestra las predicciones palabra por palabra.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"--- Prediciendo para: ---\\n{sentence_string}\\n\")\n",
    "    \n",
    "    # 0. Separamos las palabras para saber cuántas son\n",
    "    words = sentence_string.split(' ')\n",
    "    \n",
    "    # 1. Convertir la frase (string) en un tensor de IDs (shape [1, 128])\n",
    "    # El vectorizador espera una lista de strings\n",
    "    input_tensor = word_vectorizer([sentence_string])\n",
    "    \n",
    "    # 2. Obtener las predicciones del modelo\n",
    "    # El modelo devuelve probabilidades (shape [1, 128, 20])\n",
    "    predictions = model.predict(input_tensor)\n",
    "    \n",
    "    # 3. Encontrar el ID de la etiqueta con mayor probabilidad para cada token\n",
    "    # Usamos np.argmax para obtener los IDs ganadores (shape [1, 128])\n",
    "    predicted_ids = np.argmax(predictions, axis=-1)[0] # [0] para coger el primer (y único) batch\n",
    "    \n",
    "    # 4. Mostrar los resultados\n",
    "    print(\"--- Resultados: ---\")\n",
    "    for i in range(len(words)):\n",
    "        word = words[i]\n",
    "        tag_id = predicted_ids[i]\n",
    "        tag_name = tags_vocab[tag_id]\n",
    "        \n",
    "        print(f\"{word:<15} -> {tag_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef0d882-0606-4d87-8886-33d8c7e43a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Prediciendo para: ---\n",
      "Google is a nice search engine .\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 945ms/step\n",
      "--- Resultados: ---\n",
      "Google          -> propn\n",
      "is              -> aux\n",
      "a               -> det\n",
      "nice            -> adj\n",
      "search          -> noun\n",
      "engine          -> noun\n",
      ".               -> noun\n",
      "--- Prediciendo para: ---\n",
      "I am writing this new sentence for the test .\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "--- Resultados: ---\n",
      "I               -> pron\n",
      "am              -> aux\n",
      "writing         -> verb\n",
      "this            -> det\n",
      "new             -> adj\n",
      "sentence        -> noun\n",
      "for             -> adp\n",
      "the             -> det\n",
      "test            -> noun\n",
      ".               -> noun\n",
      "--- Prediciendo para: ---\n",
      "The university is in ntiago .\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "--- Resultados: ---\n",
      "The             -> det\n",
      "university      -> propn\n",
      "is              -> aux\n",
      "in              -> adp\n",
      "ntiago          -> propn\n",
      ".               -> propn\n"
     ]
    }
   ],
   "source": [
    "# Prueba 1: La frase de ejemplo de la práctica\n",
    "predict_tags(\"Google is a nice search engine .\")\n",
    "\n",
    "# Prueba 2: Una frase nueva\n",
    "predict_tags(\"I am writing this new sentence for the test .\")\n",
    "\n",
    "# Prueba 3: Otra más\n",
    "predict_tags(\"The university is in ntiago .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189ae9f-5ff1-4520-a7a9-522414995a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
