{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b508788-fc22-425d-a592-ebdeb8b4352f",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "Si hay multiwords, se debe saltar una linea y coger las dos siguientes. Ejemplo:\n",
    "\n",
    "19-20\tdon't\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "\n",
    "19\tdo\tdo\tAUX\tVBP\tMood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\t21\taux\t21:aux\t_\n",
    "\n",
    "20\tn't\tnot\tPART\tRB\tPolarity=Neg\t21\tadvmod\t21:advmod\t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e01bab38-f902-4d05-9284-738db4162657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conllu_data(filepath):\n",
    "    \"\"\"\n",
    "    Carga y procesa un archivo CoNLL-U, extrayendo las oraciones y sus etiquetas UPOS.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    current_sentence = []\n",
    "    current_tags = []\n",
    "\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # 1. Ignorar comentarios y líneas vacías que no sean separadores de oración\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "            \n",
    "            # 2. Línea en blanco: indica el final de una oración\n",
    "            elif line == '':\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    tags.append(current_tags)\n",
    "                    current_sentence = []\n",
    "                    current_tags = []\n",
    "            \n",
    "            # 3. Procesar línea de palabra\n",
    "            else:\n",
    "                fields = line.split('\\t')\n",
    "                \n",
    "                # Ignorar tokens multiword (ID con guion, e.g., '1-2') o nodos vacíos (ID con punto, e.g., '1.1') \n",
    "                if '-' in fields[0] or '.' in fields[0]:\n",
    "                    continue\n",
    "\n",
    "                # Extraer la palabra (FORM, índice 1) y la etiqueta PoS (UPOS, índice 3)\n",
    "                word = fields[1]\n",
    "                pos_tag = fields[3]\n",
    "                \n",
    "                current_sentence.append(word)\n",
    "                current_tags.append(pos_tag)\n",
    "\n",
    "    # Asegurarse de añadir la última oración si el archivo no termina en línea vacía\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    return sentences, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15447062-510f-41cc-a7ff-07ffd00199c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'a', 'nice', 'search', 'engine', '.'] ['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./en_ewt-ud-train.conllu\"\n",
    "\n",
    "sentences = []\n",
    "tags = []\n",
    "current_sentence = []\n",
    "current_tags = []\n",
    "\n",
    "with open (filepath, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Ignorar comentarios y lineas vacias\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        # Si hay una linea en blanco indica el final de una oracion\n",
    "        elif line == '':\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                tags.append(current_tags)\n",
    "                current_sentences = []\n",
    "                current_tags = []\n",
    "                \n",
    "        # Procesar línea de palabra\n",
    "        else:\n",
    "            fields = line.split('\\t')\n",
    "\n",
    "            # Ignorar tokens multiword\n",
    "            if '-' in fields[0] or '.' in fields[0]:\n",
    "                    continue\n",
    "                \n",
    "            # Extraer la palabra (FORM, índice 1) y la etiqueta PoS (UPOS, índice 3)\n",
    "            word = fields[1]\n",
    "            pos_tag = fields[3]\n",
    "                \n",
    "            current_sentence.append(word)\n",
    "            current_tags.append(pos_tag)\n",
    "\n",
    "\n",
    "    # Asegurarse de añadir la última oración si el archivo no termina en línea vacía\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    \n",
    "# Ejemplo de uso (asumiendo que los archivos están en la misma carpeta):\n",
    "train_sents, train_tags = load_conllu_data('en_ewt-ud-train.conllu')\n",
    "dev_sents, dev_tags = load_conllu_data('en_ewt-ud-dev.conllu')\n",
    "test_sents, test_tags = load_conllu_data('en_ewt-ud-test.conllu')\n",
    "\n",
    "print(test_sents[5], test_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23542ba7-4baf-48ec-b4de-ce00ba0e29f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'is', 'a', 'nice', 'search', 'engine', '.'] ['PROPN', 'AUX', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "filepath = \"./en_ewt-ud-train.conllu\"\n",
    "\n",
    "sentences = []\n",
    "tags = []\n",
    "current_sentence = []\n",
    "current_tags = []\n",
    "\n",
    "with open (filepath, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "\n",
    "        # Ignorar comentarios y lineas vacias\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "\n",
    "        # Si hay una linea en blanco indica el final de una oracion\n",
    "        elif line == '':\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                tags.append(current_tags)\n",
    "                current_sentences = []\n",
    "                current_tags = []\n",
    "                \n",
    "        # Procesar línea de palabra\n",
    "        else:\n",
    "            fields = line.split('\\t')\n",
    "\n",
    "            # Ignorar tokens multiword\n",
    "            if '-' in fields[0] or '.' in fields[0]:\n",
    "                    continue\n",
    "                \n",
    "            # Extraer la palabra (FORM, índice 1) y la etiqueta PoS (UPOS, índice 3)\n",
    "            word = fields[1]\n",
    "            pos_tag = fields[3]\n",
    "                \n",
    "            current_sentence.append(word)\n",
    "            current_tags.append(pos_tag)\n",
    "\n",
    "\n",
    "    # Asegurarse de añadir la última oración si el archivo no termina en línea vacía\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "        tags.append(current_tags)\n",
    "\n",
    "    \n",
    "# Ejemplo de uso (asumiendo que los archivos están en la misma carpeta):\n",
    "train_sents, train_tags = load_conllu_data('en_ewt-ud-train.conllu')\n",
    "dev_sents, dev_tags = load_conllu_data('en_ewt-ud-dev.conllu')\n",
    "test_sents, test_tags = load_conllu_data('en_ewt-ud-test.conllu')\n",
    "\n",
    "print(test_sents[5], test_tags[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b84413-2884-46cc-9e88-5cc63fa9dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Text Vectorization: Creating the Dictionaries\n",
    "\n",
    "The first step in preparing the data for the LSTM model is to convert our text-based sentences and tags into numerical sequences. Neural networks can only process numbers, so we need a consistent way to map each word and each tag to a unique integer ID.\n",
    "\n",
    "For this task, we'll use Keras's modern `TextVectorization` layer. We will create two separate instances of this layer: one for the input words (`word_vectorizer`) and one for the output tags (`tag_vectorizer`).\n",
    "\n",
    "The process involves two main stages:\n",
    "1.  **Configuration**: We initialize the `TextVectorization` layer with `output_mode='int'` to ensure it produces sequences of integer IDs (e.g., \"Google is nice\" -> `[2, 3, 42]`). We also set `output_sequence_length=128` to enforce that all sequences are padded or truncated to a fixed length, which is a requirement for the model.\n",
    "2.  **Adaptation**: We then call the `.adapt()` method on our training data. This step builds the internal vocabulary for each vectorizer. It analyzes all the words (or tags) in the training set and assigns a unique integer to each one. This ensures our \"dictionaries\" are based only on the data the model is allowed to learn from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e707d6-13fb-40f5-88a7-4229210c87ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example: ['Google', 'is', 'a', 'jojoto', 'engine']\n",
      "\n",
      "example vec:\n",
      "[[2475    9    5    1 1862    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Max lend of words for a sentence\n",
    "MAX_LEN = 128 \n",
    "\n",
    "# Create the TextVectorization layer.\n",
    "word_vectorizer = TextVectorization(\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN\n",
    ")\n",
    "\n",
    "#Flatten the training sentences.\n",
    "train_sents_flat = [' '.join(sentence) for sentence in train_sents]\n",
    "\n",
    "# Adapt the vectorizer to the training data.\n",
    "# This builds the internal vocabulary (the word-to-integer dictionary).\n",
    "word_vectorizer.adapt(train_sents_flat) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Let's test it with an example ---\n",
    "# Create an example sentence containing an unknown word (\"jojoto\").\n",
    "example_sentence = [\"Google\", \"is\", \"a\", \"jojoto\", \"engine\"]\n",
    "print(\"example:\", example_sentence)\n",
    "\n",
    "# running example\n",
    "example_vec = word_vectorizer([\" \".join(example_sentence)])\n",
    "print(\"\\nexample vec:\")\n",
    "print(example_vec.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f502ea4-ee7c-4470-94a3-6e8b806f4d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptation complete. Vocabulary size: 16250\n",
      "Adaptation complete. Vocabulary size: 20\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 16250 words.\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 20 tags.\n",
      "Vectorizing all data sets...\n",
      "Vectorization complete!\n",
      "\n",
      "Shape of X_train: (12544, 128)\n",
      "Shape of y_train: (12544, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "def create_and_adapt_vectorizer(sentences, max_len=128):\n",
    "\n",
    "    vectorizer = TextVectorization(\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_len\n",
    "    )\n",
    "\n",
    "    sentences_flat = [' '.join(sentence) for sentence in sentences]\n",
    "    \n",
    "    vectorizer.adapt(sentences_flat)\n",
    "    \n",
    "    vocab_size = len(vectorizer.get_vocabulary())\n",
    "    \n",
    "    print(f\"Adaptation complete. Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    return vectorizer, vocab_size\n",
    "\n",
    "# How to use the function ---\n",
    "\n",
    "\n",
    "word_vectorizer, WORD_VOCAB_SIZE = create_and_adapt_vectorizer(train_sents)\n",
    "\n",
    "tags_vectorizer, TAGS_VOCAB_SIZE = create_and_adapt_vectorizer(train_tags_flat)\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {WORD_VOCAB_SIZE} words.\")\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {TAGS_VOCAB_SIZE} tags.\")\n",
    "\n",
    "\n",
    "print(\"Vectorizing all data sets...\")\n",
    "# The vectorizer layer can be called like a function on the raw text data.\n",
    "# Note that we pass the original lists of lists (e.g., train_sents), not the flattened ones.\n",
    "train_flat = [' '.join(sentence) for sentence in train_sents]\n",
    "train_tags_flat = [' '.join(sentence) for sentence in train_tags]\n",
    "X_train = word_vectorizer(train_flat )\n",
    "y_train = tag_vectorizer(train_tags_flat)\n",
    "\n",
    "#X_dev = word_vectorizer(dev_sents)\n",
    "#y_dev = tag_vectorizer(dev_tags)\n",
    "\n",
    "# We also need to vectorize the test set for the final evaluation later\n",
    "#X_test = word_vectorizer(test_sents)\n",
    "#y_test = tag_vectorizer(test_tags)\n",
    "\n",
    "print(\"Vectorization complete!\")\n",
    "print(\"\\nShape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2915196b-9542-479f-a879-e8acf9dd8ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptation complete. Vocabulary size: 16250\n",
      "Adaptation complete. Vocabulary size: 20\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 16250 words.\n",
      "\n",
      "We have successfully created a vectorizer with a vocabulary of 20 tags.\n",
      "Vectorizing all data sets...\n",
      "Vectorization complete!\n",
      "\n",
      "Shape of X_train: (12544, 128)\n",
      "Shape of y_train: (12544, 128)\n",
      "Shape of X_dev: (2001, 128)\n",
      "Shape of y_dev: (2001, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "def create_and_adapt_vectorizer(sentences, max_len=128):\n",
    "\n",
    "    vectorizer = TextVectorization(\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_len\n",
    "    )\n",
    "\n",
    "    sentences_flat = [' '.join(sentence) for sentence in sentences]\n",
    "    \n",
    "    vectorizer.adapt(sentences_flat)\n",
    "    \n",
    "    vocab_size = len(vectorizer.get_vocabulary())\n",
    "    \n",
    "    print(f\"Adaptation complete. Vocabulary size: {vocab_size}\")\n",
    "    \n",
    "    return vectorizer, vocab_size\n",
    "\n",
    "# How to use the function ---\n",
    "\n",
    "\n",
    "word_vectorizer, WORD_VOCAB_SIZE = create_and_adapt_vectorizer(train_sents)\n",
    "\n",
    "tags_vectorizer, TAGS_VOCAB_SIZE = create_and_adapt_vectorizer(train_tags_flat)\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {WORD_VOCAB_SIZE} words.\")\n",
    "\n",
    "print(f\"\\nWe have successfully created a vectorizer with a vocabulary of {TAGS_VOCAB_SIZE} tags.\")\n",
    "\n",
    "print(\"Vectorizing all data sets...\")\n",
    "\n",
    "# --- 1. Flatten the data from list of lists to list of strings ---\n",
    "# The vectorizer layers expect a flat list of strings as input.\n",
    "train_sents_flat = [' '.join(sentence) for sentence in train_sents]\n",
    "train_tags_flat = [' '.join(tag_list) for tag_list in train_tags]\n",
    "\n",
    "dev_sents_flat = [' '.join(sentence) for sentence in dev_sents]\n",
    "dev_tags_flat = [' '.join(tag_list) for tag_list in dev_tags]\n",
    "\n",
    "test_sents_flat = [' '.join(sentence) for sentence in test_sents]\n",
    "test_tags_flat = [' '.join(tag_list) for tag_list in test_tags]\n",
    "\n",
    "\n",
    "# --- 2. Use the vectorizers to transform the flattened data ---\n",
    "# Now we call the vectorizers with the correct input format.\n",
    "X_train = word_vectorizer(train_sents_flat)\n",
    "y_train = tag_vectorizer(train_tags_flat) # <-- This is the corrected line\n",
    "\n",
    "X_dev = word_vectorizer(dev_sents_flat)\n",
    "y_dev = tag_vectorizer(dev_tags_flat)\n",
    "\n",
    "X_test = word_vectorizer(test_sents_flat)\n",
    "y_test = tag_vectorizer(test_tags_flat)\n",
    "\n",
    "print(\"Vectorization complete!\")\n",
    "print(\"\\nShape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_dev:\", X_dev.shape)\n",
    "print(\"Shape of y_dev:\", y_dev.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f9858691-93ff-41c9-8436-6750835f935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptation complete.\n",
      "Example tag sequence (original):\n",
      "['PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'ADJ', 'NOUN', 'VERB', 'PROPN', 'PROPN', 'PROPN', 'PUNCT', 'PROPN', 'PUNCT', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'PROPN', 'PUNCT', 'ADP', 'DET', 'ADJ', 'NOUN', 'PUNCT']\n",
      "\n",
      "Vectorized tag sequence:\n",
      "[[10  3 10  3  8  2  4 10 10 10  3 10  3  7  2  6  7  2  6  7  2  6 10  3\n",
      "   6  7  8  2  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Create the TextVectorization layer for the tags.\n",
    "tag_vectorizer = TextVectorization(\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LEN # Tag sequences must have the same length as word sequences.\n",
    ")\n",
    "\n",
    "# Flatten the training tags for the adaptation step.\n",
    "train_tags_flat = [' '.join(tag_list) for tag_list in train_tags]\n",
    "\n",
    "# Adapt the layer to learn the vocabulary of our training tags.\n",
    "tag_vectorizer.adapt(train_tags_flat)\n",
    "print(\"Adaptation complete.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Let's test it with an example ---\n",
    "\n",
    "# 1. Take the first list of tags from our training data as an example.\n",
    "example_tags = train_tags[0]\n",
    "print(\"Example tag sequence (original):\")\n",
    "print(example_tags)\n",
    "\n",
    "# 2. Vectorize the example tag sequence.\n",
    "# We must join it into a single string and pass it as a list.\n",
    "example_tags_vec = tag_vectorizer([\" \".join(example_tags)])\n",
    "\n",
    "# 3. Print the resulting numerical sequence.\n",
    "# Notice how the output is a sequence of 128 numbers, padded with 0s at the end.\n",
    "print(\"\\nVectorized tag sequence:\")\n",
    "print(example_tags_vec.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c66169e-26d6-48c3-92a6-fd32ac5065dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del vocabulario de palabras: 16250\n",
      "Tamaño del vocabulario de etiquetas: 19\n",
      "Algunas etiquetas del vocabulario: ['', '[UNK]', np.str_('noun'), np.str_('punct'), np.str_('verb'), np.str_('pron'), np.str_('adp'), np.str_('det'), np.str_('adj'), np.str_('aux')]\n"
     ]
    }
   ],
   "source": [
    "# get the vocab and tags\n",
    "word_vocab = word_vectorizer.get_vocabulary()\n",
    "tag_vocab = tag_vectorizer.get_vocabulary()\n",
    "\n",
    "# het the sizes\n",
    "WORD_VOCAB_SIZE = len(word_vocab)\n",
    "TAG_VOCAB_SIZE = len(tag_vocab)\n",
    "print(TAG_VOCAB_SIZE)\n",
    "print(f\"size of the worids: {WORD_VOCAB_SIZE}\")\n",
    "print(f\"size of the tags: {TAG_VOCAB_SIZE}\")\n",
    "print(f\"some tags: {tag_vocab[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59265d89-4bc5-4ed7-b03d-4dd4c8f45afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "esto es el modeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c167bc80-153b-4352-b1fd-6ac12f6b8b01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importaciones adicionales para construir el modelo\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed\n",
    "\n",
    "# Hiperparámetros del modelo (puedes experimentar con estos valores)\n",
    "EMBEDDING_DIM = 64\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "# Construcción del modelo\n",
    "model = Sequential([\n",
    "    # 1. Capa de Embedding: Convierte IDs de palabras en vectores de significado\n",
    "    Embedding(input_dim=WORD_VOCAB_SIZE, \n",
    "              output_dim=EMBEDDING_DIM, \n",
    "              mask_zero=True), # mask_zero=True le dice al modelo que ignore los '0' del padding\n",
    "\n",
    "    # 2. Capa LSTM: Procesa la secuencia de embeddings y recuerda el contexto\n",
    "    LSTM(units=LSTM_UNITS, \n",
    "         return_sequences=True), # ¡Crucial! Devuelve una salida para cada palabra\n",
    "\n",
    "    # 3. Capa de Salida: Aplica un clasificador a cada palabra de la secuencia\n",
    "    TimeDistributed(Dense(units=TAG_VOCAB_SIZE, activation='softmax'))\n",
    "])\n",
    "\n",
    "# Imprimir un resumen de la arquitectura\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eff65595-9c5c-4db4-94e2-62c5af32e129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ time_distributed_3              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importaciones adicionales para construir el modelo\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed\n",
    "\n",
    "# Hiperparámetros del modelo (puedes experimentar con estos valores)\n",
    "EMBEDDING_DIM = 64\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "# Construcción del modelo\n",
    "model = Sequential([\n",
    "    # 1. Capa de Embedding: Convierte IDs de palabras en vectores de significado\n",
    "    Embedding(input_dim=WORD_VOCAB_SIZE, \n",
    "              output_dim=EMBEDDING_DIM, \n",
    "              mask_zero=True), # mask_zero=True le dice al modelo que ignore los '0' del padding\n",
    "\n",
    "    # 2. Capa LSTM: Procesa la secuencia de embeddings y recuerda el contexto\n",
    "    LSTM(units=LSTM_UNITS, \n",
    "         return_sequences=True), # ¡Crucial! Devuelve una salida para cada palabra\n",
    "\n",
    "    # 3. Capa de Salida: Aplica un clasificador a cada palabra de la secuencia\n",
    "    TimeDistributed(Dense(units=TAG_VOCAB_SIZE, activation='softmax'))\n",
    "])\n",
    "\n",
    "# Imprimir un resumen de la arquitectura\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39f7f76f-f98e-4724-a226-e5c56c22c346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"pos_tagger_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"pos_tagger_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_ids_input      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040,000</span> │ word_ids_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ word_ids_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │ word_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_tag_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,235</span> │ lstm_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ word_ids_input      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ word_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │  \u001b[38;5;34m1,040,000\u001b[0m │ word_ids_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ word_ids_input[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_layer (\u001b[38;5;33mLSTM\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │     \u001b[38;5;34m33,024\u001b[0m │ word_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ pos_tag_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m19\u001b[0m)   │      \u001b[38;5;34m1,235\u001b[0m │ lstm_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,074,259</span> (4.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,074,259\u001b[0m (4.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,074,259</span> (4.10 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,074,259\u001b[0m (4.10 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# --- Hiperparámetros (igual que antes) ---\n",
    "# (Asegúrate de tener estas variables definidas de pasos anteriores)\n",
    "# WORD_VOCAB_SIZE, TAG_VOCAB_SIZE, MAX_LEN\n",
    "EMBEDDING_DIM = 64\n",
    "LSTM_UNITS = 64\n",
    "\n",
    "# --- Construcción del Modelo con la API Funcional ---\n",
    "\n",
    "# 1. Definir la capa de Entrada ✅\n",
    "# Le decimos al modelo que recibirá secuencias de números enteros de longitud MAX_LEN.\n",
    "inputs = Input(shape=(MAX_LEN,), name='word_ids_input')\n",
    "\n",
    "# 2. Conectar las capas en una \"carrera de relevos\" 🔗\n",
    "# La capa Embedding recibe los 'inputs' y su salida se guarda en 'x'.\n",
    "x = Embedding(\n",
    "    input_dim=WORD_VOCAB_SIZE, \n",
    "    output_dim=EMBEDDING_DIM, \n",
    "    mask_zero=True, # Importante para que ignore el padding\n",
    "    name='word_embedding'\n",
    ")(inputs)\n",
    "\n",
    "# La capa LSTM recibe la salida del Embedding ('x') y su propia salida se guarda de nuevo en 'x'.\n",
    "x = LSTM(\n",
    "    units=LSTM_UNITS, \n",
    "    return_sequences=True, # Necesitamos una salida para cada palabra\n",
    "    name='lstm_layer'\n",
    ")(x)\n",
    "\n",
    "# La capa TimeDistributed(Dense) recibe la salida de la LSTM ('x') y su salida es la final.\n",
    "# La llamamos 'outputs' para que quede claro que es el final del camino.\n",
    "outputs = TimeDistributed(\n",
    "    Dense(units=TAG_VOCAB_SIZE, activation='softmax'), \n",
    "    name='pos_tag_output'\n",
    ")(x)\n",
    "\n",
    "# 3. Crear el Modelo final ✅\n",
    "# Le decimos a Keras dónde empieza el modelo (inputs) y dónde termina (outputs).\n",
    "model = Model(inputs=inputs, outputs=outputs, name='pos_tagger_model')\n",
    "\n",
    "# ¡Listo! Ahora puedes imprimir el resumen y ver la arquitectura.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d26c495-7270-4b16-9be6-dbf849176014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar el modelo\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3d1fc2b-b956-4ced-9681-5fdd80d3d81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 87ms/step - accuracy: 0.0386 - loss: 2.2298 - val_accuracy: 0.0416 - val_loss: 1.7180\n",
      "Epoch 2/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 79ms/step - accuracy: 0.0580 - loss: 1.5977 - val_accuracy: 0.0505 - val_loss: 1.3730\n",
      "Epoch 3/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 76ms/step - accuracy: 0.0665 - loss: 1.3255 - val_accuracy: 0.0524 - val_loss: 1.2473\n",
      "Epoch 4/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 80ms/step - accuracy: 0.0715 - loss: 1.1567 - val_accuracy: 0.0530 - val_loss: 1.2091\n",
      "Epoch 5/5\n",
      "\u001b[1m196/196\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 81ms/step - accuracy: 0.0754 - loss: 1.0369 - val_accuracy: 0.0533 - val_loss: 1.2180\n"
     ]
    }
   ],
   "source": [
    "# Parámetros de entrenamiento\n",
    "EPOCHS = 5 # Empezamos con pocas para probar rápido\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_data=(X_dev, y_dev)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53a933-7fd4-4db8-a6d6-96493aa81f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
